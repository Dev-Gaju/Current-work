#### 9) Review of the workflow
This notebook based on Fashion Mnist dataset, Which have 60k train data & 10k test data with 10 classification label. Each row have 785 column which define 28x28 image size and label. After doing EDA can confirm that there is no class imballance.

In this note, Used several dimensinality method like PCA,t-SNE, FastICA, SVD, TruncateSVD with a polular clustering method named KMeans. After EDA did parameter tuning using optuna to choose the best fitted parameter for the model.Depending on finding parameter implement Kmeans on the variation of data. Firstly, just take a sample of dataset like 20k data to find the suitable reduction technique for this dataset. Finally, Got our best fitted reduction technique but not one its two. For exploring the whole dataset pick two & check which one get best fitted on that. Little bit surprised after the result. In the sample of data t-SNE technique got `0.413302` score but in whole data the score decrease to `0.3860992` but got less dcreased on the other technique which is SVD, portion of data's score was `0.0.404239453` & dataset got ` 0.402986083` which is .002 .

Many of Us mixed between feature selection & dimentionality reduction, So let's clarified that. Suppose you have a dataste & have 5 features **size, Number of rooms, Number of bathrooms, school around, crime rate**. For feature selection we can choose any of this **three** feature without changing. But When we talk about reduction its not about just chosing arbitary it about transform feature from hight to low. Again question can comes our mind, How? We have feature & obviously those have common in beteen them & we can used this trick here, for example **size, Number of rooms, Number of bathrooms** those three feature talk about size, reduction technique pick those three as **Size feature** & **school around, crime rate** these two calculate as **location**. Thats the way dimentionality reduction works from `5` columns convert it into `2`.

Now, Question is why Fashion Mnist dataset did very well on t-SNE & SVD dimentionality technique not others. Every dimentinality technique have their own paramter like PCA works good on high `variance`, `Linear Transformation` works on several reduction technique like PCA, LDA, TruncatedSVD.

- PCA skips less significant components but SVD not .
- PCA & truncate SVD are not differe so much because they are based on smae theory. we can see the score on our data.(**eigenvector, eigenvalue**), works better with better co-relation & co-variance dataset.
- t-SNE try to find the underlines structure of the datataking into the neighbors of a sample.
